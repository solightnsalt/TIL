# [CS] 캐시 메모리 (Cache Memory)

## 캐시 메모리란

+ **CPU의 속도와 시스템 메모리의 속도 차이에 기인한 문제를 해결하기 위해 CPU안에 포함되는 작고 빠른 메모리**

![cache1](https://developers.redhat.com/blog/wp-content/uploads/2016/02/will-cohen-blog_graphics-02-300x300.png)

+ 왜 필요할까? 

  + 상대적으로 빠른 CPU의 데이터가 상대적으로 느린 시스템 메모리로 넘어갈 때 시스템 메모리의 속도로는 한 번에 수용하기 어렵기 때문에 병목 현상이 발생하여 지연시간(Latency)이 생긴다.

  + 데이터가 필요할 때마다 메모리에 반복적으로 접근하게 되면 이때 발생하는 지연시간으로 인해 시스템 전체의 성능 저하를 초래할 수 있다.

  + 이와 같은 문제를 해결하기 위해서 속도의 차이를 완화 시켜 주는 중간 다리 역할로 캐시 메모리를 사용하는 

    ex. 8차선 도로에서 2차선으로 줄어드는 것을 순차적으로 8, 6, 5 ~ 2차선의 순서로 감소시키는 모습




<aside style="background-color: #f5ecc3;"> 📌 이 캐시 메모리의 효율을 높이려면 캐시 메모리에 어떤 정보가 들어있냐가 중요하다.<br/> 
    어떤 프로그램을 실행할 때 필요한 정보를 캐시 메모리에서 찾는 캐시 적중율(hit-rate)을 높일 수 있기 때문!





### 참조 지역성의 원리(principle of locality)

+ '자주 사용하는 데이터’를 판단하는 기준이 되며 캐쉬 적중률을 높일 수 있는 원리

+ 컴퓨터 프로그램이 일정 기간 동안 특정한 메모리 위치 집합에 접근하는 경향이 있는 현상

  1. 시간 지역성(temporal locality)

     최근 접근한 데이터에 다시 접근하는 경향

     ex. for나 while 같은 반복문에 사용하는 조건 변수처럼 한 번 참조된 데이터는 잠시후 또 참조될 가능성이 높음

     ```js
     // 루프에서 인덱스 역할을 하는 변수 i에는 짧은 시간안에 여러 번 접근이 이뤄진다.
     for (i = 0; i < 10; i += 1) {
       arr[i] = i;
     }
     ```

     

  2. 공간 지역성(spatial locality)

     최근 접근한 데이터의 주변 공간에 다시 접근하는 경향

     ex. 위 예시에서 배열 `arr`의 각 요소를 참조하면서 가까운 메모리 공간에 연속적으로 접근





### 동작 원리

+ CPU는 메인 메모리에서 데이터를 읽어올 때, 해당 데이터를 포함하여 참조 지역성의 원리에 따라 선택된 많은 데이터를 읽어와 캐시 메모리에 미리 Load 한다.

- 캐시 메모리에 데이터를 Load 해두면 그 다음부터는 메인 메모리에 접근하기 이전에 캐시 메모리에 찾고자 하는 데이터가 있는 경우, 캐시 메모리에서 데이터를 불러오기 때문에 메모리 접근에 필요한 지연시간을 크게 줄일 수 있다.
- 다만, 캐시 메모리에 원하는 데이터가 없을 때에는 메인 메모리에서 데이터를 새로 읽어와야 한다.
- 마지막으로 캐시 메모리가 가득 차게 되면, 새로운 데이터를 쓰기 위해서 일부 데이터를 지우게 되는데 이 때 지울 데이터의 내용이 CPU에 의해 바뀌어 있다면 같은 SDRAM 메모리 주소를 참조하는 다른 CPU와의 캐시 데이터 일관성 (Data Coherence)을 위하여 데이터를 업데이트한다.

![hitnmiss](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcr7uZA%2Fbtr2r4lTyzA%2FwuC5HNt4MGPo4M1YEc0xc0%2Fimg.png)



## Cache Metrics

캐시의 성능을 측정할 때는 히트 레이턴시(Hit latency)와 미스 레이턴시(Miss latency)가 중요한 요인으로 꼽힌다.



### Cache Hit

CPU에서 요청한 데이터가 *캐시*에 존재하는 경우

- 위치도 가깝고 CPU 내부버스를 기반으로 작동하여 빠르다

  👉 캐시히트를 하게 되면 해당 데이터를 제어장치를 거쳐 가져오게 된다.

> 캐시 적중률
>
> 수신한 요청 수와 비교하여 캐시가 성공적으로 채울 수 있는 콘텐츠 요청 수를 측정한 것
>
> ![ch](https://www.cloudflare.com/resources/images/slt3lc6tev37/9tqnmxxbqjmGcBQPvMgJN/a804f98a247f09bd21e42c398c507e96/cache-hit-ratio.svg)

- ***히트 레이턴시***

  히트가 발생해 캐싱된 데이터를 가져올 때 소요되는 시간을 의미



### Cache Miss

CPU에서 요청한 데이터가 *캐시*에 존재하지 않는 경우

+ 이 경우, 주메모리로 가서 데이터를 찾아와야하고, 메모리를 가져올때 시스템 버스를 기반으로 작동하기 때문에 느리다

+ ***미스 레이턴시*** 

  미스가 발생해 상위 캐시에서 데이터를 가져오거나(L1 캐시에 데이터가 없어서 L2 캐시에서 데이터를 찾는 경우) 메모리에서 데이터를 가져올 때 소요되는 시간

  

<aside style="background-color: #f5ecc3;"> 📌 캐시의 성능을 높이기 위해서는 캐시의 크기를 줄여 히트 레이턴시를 줄이거나, 캐시의 크기를 늘려 미스 비율을 줄이거나, 더 빠른 캐시를 이용해 레이턴시를 줄이는 방법이 있다.





## 캐시 맵핑 기법

<aside> 💡 캐시가 히트되기 위해 매핑되는 방법


- CPU의 레지스터 와 주 메모리(RAM) 간에 데이터를 주고 받을 때를 기반
- 주 메모리에 비해 굉장히 작은 레즈시터가 캐시 계층으로써 역할 -> 매핑이 중요

![mapped](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2gDIS%2FbtrTmuNp9nv%2FSKC2xePOORfdmVKzlTNKQk%2Fimg.png)

### 직접 매핑 Direct Mapped Cache

![img](https://velog.velcdn.com/images/kimmy/post/b220a36a-309b-43b0-87e8-f0d6fc9123b2/image.png)

- 가장 기본적인 구조로, DRAM의 여러 주소가 캐시 메모리의 한 주소에 대응되는 다대일 방식
- 쉽게 말하면 메인메모리와 캐시를 똑같은 크기로 나누고 순서대로 매핑하는 것

  (메모리가 1~100이 있고 캐시가 1~10이 있다면 1:1~10, 2:10~20... 와 같이 매핑)
- 간단하고 처리가 빠르지만 **Conflict miss** 발생이 잦다



### 연관 매핑 Associative Cache

- 순서를 일치하지 않고 관련 있는 캐시와 메모리를 매핑
- 충돌이 적지만 모든 블록을 탐색하여 속도가 느리다



#### 연관 매핑 Fully Associative Cache

- 순서를 일치하지 않고 필요한 메모리값을 비어있는 캐시 메모리의 어디든 편하게 매핑
- 저장할 때는 매우 간단하고 충돌이 적지만 데이터를 찾을 때는 모든 블록을 탐색하기 때문에 찾는 과정은 복잡하고 속도가 느릴 수 밖에 없다.
- but, 조건이나 규칙이 없어서 특정 캐시 Set안에 있는 모든 블럭을 한 번에 찾아 원하는 데이터가 있는지 탐색해야 함

- C<u>AM이라는 특수한 메모리 구조를 사용해야 하지만 가격이 매우 비쌈</u>

> 정말 필요한 캐시들 위주로 저장할 수 있기 때문에 적중률은 높다. 
>
> 캐시가 일반 메모리보다 속도가 훨씬 빠르므로 캐시의 검색량을 신경쓰는 것 보단 적중률이 높은게 성능이 더 좋다고 한다.



#### 직접 연관 매핑 Set Associated Cache

- 연관매핑에 직접매핑을 합쳐 놓은 방식
- 특정 행을 지정하고 순서를 일치시키되, 일정 그룹을 두어 그룹 내에서는 행안의 어떤 열이든 비어있을 때 편하게 저장하는 방식

  (메모리가 1~100까지 있고 캐시가 1~10까지 있다면 캐시 1~5에는 1~50의 데이터를 무작위로 매핑)
- Direct에 비해 검색 속도는 느리지만, 저장이 빠르고 Fully에 비해 저장이 느린 대신 검색이 빠른 중간형
- 블록화가 되어 있기 때문에 검색은 좀 더 효율적으로 되고 직접매핑처럼 저장위치에 대한 큰 제약이 있는건 아니기 때문에 적중률이 많이 떨어지지도 않는다. 





> 참조
>
> https://shuu.tistory.com/49
>
> https://parksb.github.io/article/29.html
>
> [캐시메모리의-개념과-매핑기법에-대한-설명](https://gguljaem.tistory.com/entry/%EC%BA%90%EC%8B%9C%EB%A9%94%EB%AA%A8%EB%A6%AC%EC%9D%98-%EA%B0%9C%EB%85%90%EA%B3%BC-%EB%A7%A4%ED%95%91%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%84%A4%EB%AA%85)
>
> https://ttl-blog.tistory.com/1095